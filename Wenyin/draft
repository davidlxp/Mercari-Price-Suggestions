# Install all required packages.
install.packages(c("data.table","ggplot2", "e1071", "caret", 
                   "irlba", "randomForest","janeaustenr","tidytext","dplyr","stringr","quanteda"))
library("data.table")
library("tokenizers")
library(dplyr) # data manipulation
library(tidyr) # data manipulation
library(tibble) # data wrangling
library(stringr) # String processing

#read data
train <- as.data.frame(fread("train.tsv"))
#generate 5000 sample randomly
train_sample<- train[sample(nrow(train), 7000), ] 

# Use caret to create a 70%/30% stratified split.
library(caret)
indexes <- createDataPartition(train_sample$price, times = 1,
                               p = 0.7, list = FALSE)

sample_train <- train_sample[indexes,]
sample_test <- train_sample[-indexes,]


######### Processing Price, item_condition_id, shipping ############################
#modifying/quantifying raw data 
sample_train = sample_train %>% mutate(log_price = log(price+1)) # take log of the price
sample_train = sample_train %>% mutate(item_condition_id = factor(item_condition_id))
sample_train = sample_train %>% mutate(shipping = factor(shipping))
sample_train = sample_train %>% mutate(num_token_des = str_count(item_description, '\\S+'))
sample_train = sample_train %>% mutate(num_token_name = str_count(name, '\\S+'))
sample_train = data.frame(sample_train, str_split_fixed(sample_train$category_name, '/', 4)) %>%
  mutate(cat1=X1, cat2=X2, cat3=X3, cat4=X4) %>% select(-X1, -X2, -X3, -X4)


#selecting features for regression
trial=sample_train[,c("item_condition_id","cat1","cat2","cat3","cat4","brand_name","shipping","num_token_des","num_token_name","log_price")]
trial1=sparse.model.matrix(~item_condition_id+cat1+cat2+cat3+cat4+brand_name+shipping+num_token_des+num_token_name,trial[,1:(ncol(trial)-1)])

library(dplyr) # data manipulation
library(tidyr) # data manipulation
library(tibble) # data wrangling
library(quanteda)
#processing descrription/name
#des_tfidf <- dfm_tfidf(dfm_trim(dfm(tokens(tokens_remove(tokens(corpus(char_tolower(sample_train$item_description)),   
                                                                remove_numbers = FALSE,remove_punct = TRUE,remove_symbols = TRUE,remove_separators = TRUE),stopwords("english")), 
                                           ngrams = 1:2)), min_count = 600))
#name_tfidf <- dfm_tfidf(dfm_trim(dfm(tokens(tokens_remove(tokens(corpus(char_tolower(sample_train$name)),
                                                                 remove_numbers = TRUE,remove_punct = TRUE,remove_symbols = TRUE,remove_separators = TRUE),stopwords("english")), 
                                            ngrams = 1)), min_count = 30))





class(des_tfidf) <- class(trial1) #matrix
class(name_tfidf) <- class(trial1) #matrix
modtrain=cbind(trial1,des_tfidf,name_tfidf)


# Tokenize for description
library(quanteda)
train.tokens <- tokens(sample_train$item_description, what = "word", 
                       remove_numbers = TRUE, remove_punct = TRUE,
                       remove_symbols = TRUE, remove_hyphens = TRUE)
train.tokens <- tokens_tolower(train.tokens)
train.tokens <- tokens_select(train.tokens, stopwords(), 
                              selection = "remove")
train.tokens <- tokens_wordstem(train.tokens, language = "english")
# Create our first bag-of-words model.ï¼ˆmatrix of words frequncy)
train.tokens.dfm <- dfm(train.tokens, tolower = FALSE)
# Transform to a matrix and inspect.
train.tokens.matrix <- as.matrix(train.tokens.dfm)
#View(train.tokens.matrix[1:20, 1:100])
dim(train.tokens.matrix)
#calculating relative term frequency (TF)
term.frequency <- function(row) {
  row / sum(row)
}
#calculating inverse document frequency (IDF)
inverse.doc.freq <- function(col) {
  corpus.size <- length(col)
  doc.count <- length(which(col > 0))
  
  log10(corpus.size / doc.count)
}
# Our function for calculating TF-IDF.
tf.idf <- function(x, idf) {
  x * idf
}
# First step, normalize all documents via TF.
train.tokens.df <- apply(train.tokens.matrix, 1, term.frequency)
#dim(train.tokens.df)
#View(train.tokens.df[1:20, 1:100])
# Second step, calculate the IDF vector that we will use - both
# for training data and for test data!
train.tokens.idf <- apply(train.tokens.matrix, 2, inverse.doc.freq)
str(train.tokens.idf)
# Lastly, calculate TF-IDF for our training corpus.
train.tokens.tfidf <-  apply(train.tokens.df, 2, tf.idf, idf = train.tokens.idf)
#dim(train.tokens.tfidf)
#View(train.tokens.tfidf[1:25, 1:25])
# Transpose the matrix
train.tokens.tfidf <- t(train.tokens.tfidf)
#dim(train.tokens.tfidf)
#View(train.tokens.tfidf[1:25, 1:25])
# Check for incopmlete cases.
incomplete.cases <- which(!complete.cases(train.tokens.tfidf))
sample_train$item_description[incomplete.cases]
# Fix incomplete cases
train.tokens.tfidf[incomplete.cases,] <- rep(0.0, ncol(train.tokens.tfidf))
dim(train.tokens.tfidf)
sum(which(!complete.cases(train.tokens.tfidf)))

#SVD
library(irlba)
# Time the code execution
#start.time <- Sys.time()
#reduce dimensionality down to 300 columns for our latent semantic analysis (LSA).
train.irlba_des <- irlba(t(train.tokens.tfidf), nv = 300, maxit = 600)

# Total time of execution on workstation was 
#total.time <- Sys.time() - start.time
#total.time
# Take a look at the new feature data up close.
#View(train.irlba$v)


# As with TF-IDF, we will need to project new data (e.g., the test data)
# into the SVD semantic space. The following code illustrates how to do
# this using a row of the training data that has already been transformed
# by TF-IDF
sigma.inverse <- 1 / train.irlba_des$d
u.transpose <- t(train.irlba_des$u)
document <- train.tokens.tfidf[1,]
document.hat <- sigma.inverse * u.transpose %*% document

# Look at the first 10 components of projected document and the corresponding
# row in our document semantic space (i.e., the V matrix)
document.hat[1:10]
train.irlba_des$v[1, 1:100]
